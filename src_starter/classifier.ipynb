{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# to do all the loss calculations, since automatic gradients are needed\n",
    "import numpy as np\n",
    "\n",
    "# Use helper packages\n",
    "from AbstractBaseCollabFilterSGD import AbstractBaseCollabFilterSGD\n",
    "from train_valid_test_loader import load_train_valid_test_datasets\n",
    "\n",
    "from surprise import SVD\n",
    "from surprise import Dataset, Reader, accuracy\n",
    "from surprise.model_selection import cross_validate\n",
    "\n",
    "# Some packages you might need (uncomment as necessary)\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "print(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global mean:\n",
      "3.529480398257623\n",
      "shape of bias_per_item: \n",
      "(1662,)\n",
      "shape of bias_per_user: \n",
      "(943,)\n",
      "shape of U (per user vectors): \n",
      "(943, 50)\n",
      "shape of V (per item vectors): \n",
      "(1662, 50)\n",
      "user info:  (943, 4)\n",
      "movie shape:  (1681, 3)\n",
      "Movies with ratings retained: (1662, 3)\n"
     ]
    }
   ],
   "source": [
    "## Load the entire dev set in surprise's format\n",
    "reader = Reader(\n",
    "    line_format='user item rating', sep=',',\n",
    "    rating_scale=(1, 5), skip_lines=1)\n",
    "\n",
    "train_set = Dataset.load_from_file(\n",
    "    '../data_movie_lens_100k/ratings_all_development_set.csv', reader=reader)\n",
    "\n",
    "train_set = train_set.build_full_trainset()\n",
    "\n",
    "# Use the SVD algorithm\n",
    "n_factors = 50\n",
    "## Fit model like our M3\n",
    "model = SVD(n_factors=n_factors)\n",
    "model.fit(train_set)\n",
    "\n",
    "print(\"global mean:\")\n",
    "print(model.trainset.global_mean)\n",
    "print(\"shape of bias_per_item: \")\n",
    "print(model.bi.shape)\n",
    "print(\"shape of bias_per_user: \")\n",
    "print(model.bu.shape)\n",
    "print(\"shape of U (per user vectors): \")\n",
    "print(model.pu.shape) # pu is the user vector (ui)\n",
    "print(\"shape of V (per item vectors): \")\n",
    "print(model.qi.shape) # qi is the item vector (uv)\n",
    "\n",
    "\n",
    "# \n",
    "user_info_df = pd.read_csv('../data_movie_lens_100k/user_info.csv')\n",
    "movie_info_df = pd.read_csv('../data_movie_lens_100k/movie_info.csv')\n",
    "movie_info_df = movie_info_df.drop('title', axis=1)\n",
    "\n",
    "print(\"user info: \", user_info_df.shape)\n",
    "print(\"movie shape: \", movie_info_df.shape)\n",
    "\n",
    "# compare shapes\n",
    "\n",
    "rated_movies = train_set.all_items()\n",
    "rated_movie_ids = [int(train_set.to_raw_iid(i)) for i in rated_movies]  # Convert to raw ids\n",
    "\n",
    "# Keep only movies in movie_info_df that have ratings\n",
    "clean_movie_info_df = movie_info_df[movie_info_df['item_id'].isin(rated_movie_ids)]\n",
    "print(f\"Movies with ratings retained: {clean_movie_info_df.shape}\")\n",
    "\n",
    "\n",
    "# rename it so it's usable\n",
    "users_U = model.pu\n",
    "movies_V = model.qi\n",
    "user_features = user_info_df\n",
    "movie_features = clean_movie_info_df\n",
    "\n",
    "ratings_y = train_set.all_ratings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline:\n",
    "\n",
    "# \n",
    "# collapse U, V, user_feature, item_feature into one matrix to train\n",
    "\n",
    "# put U and user_features together:\n",
    "# Prepare user features\n",
    "user_ids = list(range(0, 942))  # list of all user inner ids\n",
    "user_features_list = []\n",
    "for inner_id in user_ids:\n",
    "    user_vector = users_U[inner_id]\n",
    "    user_id = model.trainset.to_raw_uid(inner_id)\n",
    "    user_meta = user_info_df.loc[user_info_df['user_id'] == int(user_id)].drop('user_id', axis=1)\n",
    "    if not user_meta.empty:\n",
    "        combined_features = np.concatenate([user_vector, user_meta.iloc[0].values])\n",
    "        user_features_list.append(combined_features)\n",
    "\n",
    "# Convert to DataFrame\n",
    "user_features_df = pd.DataFrame(user_features_list, columns=[f'feature_{i}' for i in range(len(user_features_list[0]))])\n",
    "\n",
    "# put V and item_features together:\n",
    "# Prepare item features\n",
    "item_ids = list(range(model.trainset.n_items))  # list of all item inner ids\n",
    "item_features_list = []\n",
    "for inner_id in item_ids:\n",
    "    item_vector = model.qi[inner_id]\n",
    "    item_id = model.trainset.to_raw_iid(inner_id)\n",
    "    item_meta = movie_features.loc[movie_features['item_id'] == int(item_id)].drop('item_id', axis=1)\n",
    "    if not item_meta.empty:\n",
    "        combined_features = np.concatenate([item_vector, item_meta.iloc[0].values])\n",
    "        item_features_list.append(combined_features)\n",
    "\n",
    "# Convert to DataFrame\n",
    "item_features_df = pd.DataFrame(item_features_list, columns=[f'feature_{i}' for i in range(len(item_features_list[0]))])\n",
    "\n",
    "\n",
    "\n",
    "# after that for predcitions:\n",
    "\n",
    "# on prediction:\n",
    "    # take in user_id, item_id\n",
    "    # get U and V from SVG algorithm \n",
    "    # get corresponding features of users/items\n",
    "# condense the features and put into classifier for prediction\n",
    "\n",
    "# get output of at least 4.5\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   user_id  item_id  rating  feature_0_x  feature_1_x  feature_2_x  \\\n",
      "0      772       36       3     0.000519    -0.018147    -0.010226   \n",
      "1      471      228       5    -0.117474    -0.065134     0.133078   \n",
      "2      641      401       4     0.030019    -0.019611     0.179792   \n",
      "3      312       98       4     0.268444    -0.141399    -0.150881   \n",
      "4       58      504       5    -0.178349    -0.058808     0.142644   \n",
      "\n",
      "   feature_3_x  feature_4_x  feature_5_x  feature_6_x  ...  feature_42_y  \\\n",
      "0     0.192360    -0.209623    -0.008063     0.001900  ...      0.118245   \n",
      "1     0.057704     0.008165     0.064068    -0.133663  ...     -0.047907   \n",
      "2    -0.095667    -0.018864     0.261649     0.002808  ...     -0.065106   \n",
      "3     0.110825    -0.042200     0.453833    -0.172197  ...      0.342833   \n",
      "4    -0.073788     0.099370     0.134943    -0.149083  ...      0.053106   \n",
      "\n",
      "   feature_43_y  feature_44_y  feature_45_y  feature_46_y  feature_47_y  \\\n",
      "0      0.168422      0.117078     -0.053176     -0.038728      0.046947   \n",
      "1     -0.030450      0.248421      0.086144     -0.115991      0.202910   \n",
      "2      0.066414     -0.186972      0.045653      0.125325     -0.041283   \n",
      "3     -0.141208      0.154557      0.040296     -0.100801     -0.160926   \n",
      "4     -0.012984     -0.049853      0.000038     -0.067223      0.279883   \n",
      "\n",
      "   feature_48_y  feature_49_y  feature_50_y  feature_51_y  \n",
      "0     -0.149263      0.075800        1956.0         614.0  \n",
      "1     -0.096538     -0.243238        1996.0         221.0  \n",
      "2      0.051445      0.093642        1994.0        1419.0  \n",
      "3     -0.035576      0.004683        1984.0         191.0  \n",
      "4     -0.217751     -0.087487        1994.0          46.0  \n",
      "\n",
      "[5 rows x 108 columns]\n"
     ]
    }
   ],
   "source": [
    "# Generate user-item pairs with known ratings from your ratings dataset\n",
    "ratings_df = pd.read_csv('../data_movie_lens_100k/ratings_all_development_set.csv')\n",
    "\n",
    "# Merge user and item features into the ratings DataFrame\n",
    "ratings_df = ratings_df.merge(user_features_df, left_on='user_id', right_index=True, how='left')\n",
    "ratings_df = ratings_df.merge(item_features_df, left_on='item_id', right_index=True, how='left')\n",
    "\n",
    "# print(ratings_df[0:5])\n",
    "\n",
    "# Finding rows where any column has NaN\n",
    "\n",
    "# Substitute NaN values with 0 in the entire DataFrame\n",
    "ratings_df.fillna(0, inplace=True)\n",
    "\n",
    "ratings_df.fillna(0, inplace=True)\n",
    "cleaned_ratings = ratings_df\n",
    "# Optionally, check the first few rows to confirm the substitution\n",
    "print(ratings_df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error: 1.2201700094449692\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "feature_cols = [col for col in ratings_df.columns if col.startswith('feature')]\n",
    "\n",
    "# Prepare data\n",
    "X = cleaned_ratings[feature_cols]  # all your feature columns\n",
    "y = cleaned_ratings['rating']      # or a binary column if you're classifying\n",
    "\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# Create a scaler object\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit on training data and transform both training and testing data\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "# Initialize the KNeighborsRegressor\n",
    "knn = KNeighborsRegressor(n_neighbors=5)\n",
    "\n",
    "# Train the model\n",
    "knn.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred = knn.predict(X_test_scaled)\n",
    "\n",
    "# Calculate the RMSE\n",
    "rmse = np.mean(np.square(y_pred - y_test))\n",
    "print(f'Root Mean Squared Error: {rmse}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 105)\n"
     ]
    }
   ],
   "source": [
    "# make prediction \n",
    "\n",
    "# pass in item_id, user_id into cool matrix thing\n",
    "# Generate user-item pairs with known ratings from your ratings dataset\n",
    "test_ratings_df = pd.read_csv('../data_movie_lens_100k/ratings_masked_leaderboard_set.csv')\n",
    "\n",
    "# Merge user and item features into the ratings DataFrame\n",
    "test_ratings_df = test_ratings_df.merge(user_features_df, left_on='user_id', right_index=True, how='left')\n",
    "test_ratings_df = test_ratings_df.merge(item_features_df, left_on='item_id', right_index=True, how='left')\n",
    "\n",
    "# Finding rows where any column has NaN\n",
    "# cleaned_ratings = ratings_df.dropna()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X_test_true = test_ratings_df[feature_cols]\n",
    "X_test_true = X_test_true.fillna(0)\n",
    "print(X_test_true.shape)\n",
    "y_test_true = test_ratings_df['rating']\n",
    "X_test_true_scaled = scaler.transform(X_test_true)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred = knn.predict(X_test_true_scaled)\n",
    "\n",
    "predicted_df = pd.read_csv('../data_movie_lens_100k/ratings_masked_leaderboard_set.csv')\n",
    "\n",
    "predicted_df['rating'] = y_pred\n",
    "\n",
    "binary_df = {\n",
    "    'binary' : []\n",
    "}\n",
    "\n",
    "predicted_df['rating'] = predicted_df['rating'].astype(float)\n",
    "# Create a new 'binary' column based on the condition\n",
    "predicted_df['binary'] = np.where(predicted_df['rating'] >= 4.5, 1, 0)\n",
    "# Export the DataFrame to a new CSV file\n",
    "predicted_df['binary'].to_csv('predictions.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs135_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
